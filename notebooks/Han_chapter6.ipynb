{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirabbasgashtil/data-mining-course/blob/main/notebooks/Han_chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qSrRfTnhRl0"
      },
      "source": [
        "# Han - classification, basic concepts and methods - chapter 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHr9XPUMhtBP"
      },
      "source": [
        "## decision tree induction\n",
        "Decision tree induction is the learning of decision trees from class-labeled training tuples. A decision\n",
        "tree is a flowchart-like tree structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbUn1cW3g6wl",
        "outputId": "5c29dd25-adf2-4ace-86ff-7913fe15137b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.94\n",
            "Decision Tree Rules:\n",
            " |--- color_intensity <= 3.82\n",
            "|   |--- proline <= 1002.50\n",
            "|   |   |--- ash <= 3.07\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- ash >  3.07\n",
            "|   |   |   |--- class: 0\n",
            "|   |--- proline >  1002.50\n",
            "|   |   |--- class: 0\n",
            "|--- color_intensity >  3.82\n",
            "|   |--- flavanoids <= 1.40\n",
            "|   |   |--- class: 2\n",
            "|   |--- flavanoids >  1.40\n",
            "|   |   |--- proline <= 724.50\n",
            "|   |   |   |--- malic_acid <= 3.92\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- malic_acid >  3.92\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |--- proline >  724.50\n",
            "|   |   |   |--- class: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "accuracy = classifier.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "tree_rules = export_text(classifier, feature_names=wine.feature_names)\n",
        "print(\"Decision Tree Rules:\\n\", tree_rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckJTZVsgkV7x"
      },
      "source": [
        "## Attribute selection measures\n",
        "An attribute selection measure is a heuristic for selecting the splitting criterion that “best” separates\n",
        "a given data partition, D, of class-labeled training tuples into individual classes.\n",
        "\n",
        "**Information Gain** Measures how much \"information\" or \"uncertainty\" is reduced by choosing a particular attribute.\n",
        "\n",
        "**Gain Ratio** A refinement of information gain that avoids a bias toward attributes with many distinct values\n",
        "\n",
        "**Gini Index** Measures the “impurity” of a dataset; it quantifies how mixed the classes are within the subsets after a split.\n",
        "\n",
        "**Chi-Square** A statistical measure to test the independence of an attribute with respect to the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k2JkzkxwsKf",
        "outputId": "123acc31-1acf-4317-e7f1-3a4c5705d790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Decision Tree Rules for Selected Features:\n",
            " |--- color_intensity <= 3.82\n",
            "|   |--- proline <= 1002.50\n",
            "|   |   |--- proline <= 790.00\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- proline >  790.00\n",
            "|   |   |   |--- color_intensity <= 3.46\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- color_intensity >  3.46\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |--- proline >  1002.50\n",
            "|   |   |--- class: 0\n",
            "|--- color_intensity >  3.82\n",
            "|   |--- proline <= 755.00\n",
            "|   |   |--- color_intensity <= 4.85\n",
            "|   |   |   |--- proline <= 517.50\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- proline >  517.50\n",
            "|   |   |   |   |--- proline <= 645.00\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |   |   |   |--- proline >  645.00\n",
            "|   |   |   |   |   |--- color_intensity <= 3.88\n",
            "|   |   |   |   |   |   |--- class: 2\n",
            "|   |   |   |   |   |--- color_intensity >  3.88\n",
            "|   |   |   |   |   |   |--- proline <= 670.00\n",
            "|   |   |   |   |   |   |   |--- class: 1\n",
            "|   |   |   |   |   |   |--- proline >  670.00\n",
            "|   |   |   |   |   |   |   |--- color_intensity <= 4.11\n",
            "|   |   |   |   |   |   |   |   |--- class: 1\n",
            "|   |   |   |   |   |   |   |--- color_intensity >  4.11\n",
            "|   |   |   |   |   |   |   |   |--- class: 0\n",
            "|   |   |--- color_intensity >  4.85\n",
            "|   |   |   |--- proline <= 472.50\n",
            "|   |   |   |   |--- proline <= 437.50\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |   |   |   |--- proline >  437.50\n",
            "|   |   |   |   |   |--- class: 1\n",
            "|   |   |   |--- proline >  472.50\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |--- proline >  755.00\n",
            "|   |   |--- color_intensity <= 7.35\n",
            "|   |   |   |--- class: 0\n",
            "|   |   |--- color_intensity >  7.35\n",
            "|   |   |   |--- proline <= 1070.00\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |   |--- proline >  1070.00\n",
            "|   |   |   |   |--- class: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use Information Gain for feature selection\n",
        "selector = SelectKBest(score_func=chi2, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the classifier to the selected training data\n",
        "clf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions on the selected test data\n",
        "predictions = clf.predict(X_test_selected)\n",
        "\n",
        "# Display the accuracy of the model\n",
        "accuracy = clf.score(X_test_selected, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the decision tree rules for the selected features\n",
        "selected_feature_names = [wine.feature_names[i] for i in selector.get_support(indices=True)]\n",
        "tree_rules = export_text(clf, feature_names=selected_feature_names)\n",
        "print(\"Decision Tree Rules for Selected Features:\\n\", tree_rules)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlb56Db7z_Hh"
      },
      "source": [
        "## tree pruning\n",
        "Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDQJJY0Dzrbu",
        "outputId": "6401cf53-9b5b-4264-848a-4e896165fbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassifier(ccp_alpha=0.001)\n",
            "Accuracy: 0.94\n",
            "Decision Tree Rules for Pruned Tree:\n",
            " |--- color_intensity <= 3.82\n",
            "|   |--- proline <= 1002.50\n",
            "|   |   |--- ash <= 3.07\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- ash >  3.07\n",
            "|   |   |   |--- class: 0\n",
            "|   |--- proline >  1002.50\n",
            "|   |   |--- class: 0\n",
            "|--- color_intensity >  3.82\n",
            "|   |--- flavanoids <= 1.40\n",
            "|   |   |--- class: 2\n",
            "|   |--- flavanoids >  1.40\n",
            "|   |   |--- proline <= 724.50\n",
            "|   |   |   |--- alcohol <= 13.14\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- alcohol >  13.14\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |--- proline >  724.50\n",
            "|   |   |   |--- class: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Define the hyperparameter grid for cost-complexity pruning\n",
        "param_grid = {'ccp_alpha': [0.001, 0.002, 0.003, 0.004, 0.005]}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameter\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best decision tree classifier\n",
        "best_clf = grid_search.best_estimator_\n",
        "print(best_clf)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = best_clf.predict(X_test)\n",
        "\n",
        "# Display the accuracy of the pruned tree\n",
        "accuracy = best_clf.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the decision tree rules for the pruned tree\n",
        "tree_rules = export_text(best_clf, feature_names=wine.feature_names)\n",
        "print(\"Decision Tree Rules for Pruned Tree:\\n\", tree_rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTaXYp0o0szA"
      },
      "source": [
        "## bayes theorem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y4bgOBe1QKi",
        "outputId": "13b38128-0fb1-4486-f887-369840692485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "%pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVCKlxY20nVo",
        "outputId": "63023515-f6be-43dd-fc66-e5387e190bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
            "0            0.00               0.64           0.64           0.0   \n",
            "1            0.21               0.28           0.50           0.0   \n",
            "2            0.06               0.00           0.71           0.0   \n",
            "3            0.00               0.00           0.00           0.0   \n",
            "4            0.00               0.00           0.00           0.0   \n",
            "\n",
            "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
            "0           0.32            0.00              0.00                0.00   \n",
            "1           0.14            0.28              0.21                0.07   \n",
            "2           1.23            0.19              0.19                0.12   \n",
            "3           0.63            0.00              0.31                0.63   \n",
            "4           0.63            0.00              0.31                0.63   \n",
            "\n",
            "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
            "0             0.00            0.00  ...                   0.0         0.00   \n",
            "1             0.00            0.94  ...                   0.0         0.00   \n",
            "2             0.64            0.25  ...                   0.0         0.01   \n",
            "3             0.31            0.63  ...                   0.0         0.00   \n",
            "4             0.31            0.63  ...                   0.0         0.00   \n",
            "\n",
            "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
            "0        0.000          0.0        0.778        0.000        0.000   \n",
            "1        0.132          0.0        0.372        0.180        0.048   \n",
            "2        0.143          0.0        0.276        0.184        0.010   \n",
            "3        0.137          0.0        0.137        0.000        0.000   \n",
            "4        0.135          0.0        0.135        0.000        0.000   \n",
            "\n",
            "   capital_run_length_average  capital_run_length_longest  \\\n",
            "0                       3.756                          61   \n",
            "1                       5.114                         101   \n",
            "2                       9.821                         485   \n",
            "3                       3.537                          40   \n",
            "4                       3.537                          40   \n",
            "\n",
            "   capital_run_length_total  \n",
            "0                       278  \n",
            "1                      1028  \n",
            "2                      2259  \n",
            "3                       191  \n",
            "4                       191  \n",
            "\n",
            "[5 rows x 57 columns]\n",
            " \n",
            "Accuracy: 0.79\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       531\n",
            "           1       0.76      0.72      0.74       390\n",
            "\n",
            "    accuracy                           0.79       921\n",
            "   macro avg       0.78      0.78      0.78       921\n",
            "weighted avg       0.79      0.79      0.79       921\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# load spambase dataset\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "X = spambase.data.features\n",
        "y = spambase.data.targets\n",
        "print(X.head())\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Naive Bayes classifier (MultinomialNB for discrete features)\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\" \\nAccuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsfFAxWK2lCS"
      },
      "source": [
        "## Naïve Bayesian classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIELAaOJ1SQP",
        "outputId": "3c6bca64-7909-4886-909f-b75185e2d279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.66\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.86      0.12      0.21       151\n",
            "           comp.graphics       0.70      0.61      0.66       202\n",
            " comp.os.ms-windows.misc       0.67      0.61      0.64       195\n",
            "comp.sys.ibm.pc.hardware       0.52      0.78      0.62       183\n",
            "   comp.sys.mac.hardware       0.89      0.64      0.74       205\n",
            "          comp.windows.x       0.89      0.81      0.85       215\n",
            "            misc.forsale       0.86      0.60      0.71       193\n",
            "               rec.autos       0.85      0.73      0.79       196\n",
            "         rec.motorcycles       0.51      0.74      0.61       168\n",
            "      rec.sport.baseball       0.96      0.77      0.86       211\n",
            "        rec.sport.hockey       0.88      0.88      0.88       198\n",
            "               sci.crypt       0.63      0.83      0.71       201\n",
            "         sci.electronics       0.85      0.55      0.67       202\n",
            "                 sci.med       0.88      0.69      0.77       194\n",
            "               sci.space       0.82      0.74      0.78       189\n",
            "  soc.religion.christian       0.26      0.96      0.41       202\n",
            "      talk.politics.guns       0.76      0.76      0.76       188\n",
            "   talk.politics.mideast       0.79      0.77      0.78       182\n",
            "      talk.politics.misc       0.95      0.13      0.23       159\n",
            "      talk.religion.misc       1.00      0.01      0.01       136\n",
            "\n",
            "                accuracy                           0.66      3770\n",
            "               macro avg       0.78      0.64      0.63      3770\n",
            "            weighted avg       0.77      0.66      0.65      3770\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create a Naïve Bayes classifier (MultinomialNB for discrete features)\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the TF-IDF transformed training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the TF-IDF transformed test data\n",
        "predictions = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=newsgroups.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCfXU-i_5GX8"
      },
      "source": [
        "## k-nearest-neighbor classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66BDUcbs4Xhe",
        "outputId": "d1db0140-8cae-4937-b6c9-8ac5a87373ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     class_0       0.86      0.86      0.86        14\n",
            "     class_1       0.92      0.79      0.85        14\n",
            "     class_2       0.60      0.75      0.67         8\n",
            "\n",
            "    accuracy                           0.81        36\n",
            "   macro avg       0.79      0.80      0.79        36\n",
            "weighted avg       0.82      0.81      0.81        36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a k-NN classifier with k=3\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=wine.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vshPUa-t6csA"
      },
      "source": [
        "## Case-based reasoning\n",
        "Case-based reasoning (CBR) is a problem-solving technique that uses past experiences to solve new problems. It's a machine learning technique that's similar to analogical processing and can be used in a variety of applications, including medicine, law, and computer algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "NDbKV7_w6Lp6",
        "outputId": "f1214cdf-3ee4-4f9f-d0fe-3505c51a0e09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3d152914-fcc2-4cb4-846f-f1a6f159f41f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3d152914-fcc2-4cb4-846f-f1a6f159f41f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import files\n",
        "\n",
        "# Load the MovieLens dataset\n",
        "uploaded = files.upload()\n",
        "movies = pd.read_csv('movies.csv')\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "print(movies.head())\n",
        "print(ratings.head())\n",
        "\n",
        "# Merge movies and ratings data\n",
        "movie_ratings = pd.merge(ratings, movies, on='movieId')\n",
        "print(movie_ratings.head())\n",
        "\n",
        "# Create a user-item matrix for collaborative filtering\n",
        "user_item_matrix = movie_ratings.pivot_table(index='userId', columns='title', values='rating', fill_value=0)\n",
        "\n",
        "# Use TF-IDF to convert movie titles into numerical features\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['title'])\n",
        "\n",
        "# Calculate cosine similarity between movie titles\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Function to get movie recommendations using CBR\n",
        "def get_movie_recommendations(movie_title):\n",
        "    movie_indices = movies.index[movies['title'] == movie_title].tolist()\n",
        "    if not movie_indices:\n",
        "        print(f\"Movie '{movie_title}' not found in the dataset.\")\n",
        "        return []\n",
        "    movie_index = movie_indices[0]\n",
        "    cosine_scores = list(enumerate(cosine_sim[movie_index]))\n",
        "    cosine_scores = sorted(cosine_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_similar_movies = cosine_scores[1:6]  # 5 similar movies Exclude the input movie itself\n",
        "\n",
        "    recommended_movies = []\n",
        "    for index in top_similar_movies:\n",
        "        recommended_movies.append(movies['title'].iloc[index])\n",
        "\n",
        "    return recommended_movies\n",
        "\n",
        "# Example usage\n",
        "input_movie = \"Toy Story (1995)\"\n",
        "recommendations = get_movie_recommendations(input_movie)\n",
        "\n",
        "# Display the recommendations\n",
        "print(f\"\\n\\nMovies similar to '{input_movie}':\")\n",
        "for movie in recommendations:\n",
        "    print(\"-\", movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlQ8yRkeNEt"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmzX9A4b7SM0"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# California Housing dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.DataFrame(housing.target, columns=['target'])\n",
        "print(X.head())\n",
        "\n",
        "# median income in block group feature => MedInc\n",
        "X_feature = X[['MedInc']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the model coefficients and performance metrics\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "# Plot the regression line\n",
        "plt.scatter(X_test, y_test, color='black')\n",
        "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
        "plt.xlabel('Average Number of Rooms (RM)')\n",
        "plt.ylabel('House Price')\n",
        "plt.title('Linear Regression: House Price Prediction')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-UnMUJgoxd"
      },
      "source": [
        "## Perceptron: turning linear regression to classification\n",
        "Perceptron is Machine Learning algorithm for supervised learning of various binary classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ROhvYP5eyw1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Consider only the first two features for simplicity and binary classification\n",
        "X = X[:, :2]\n",
        "\n",
        "# Map wine classes to binary classes (setosa vs. non-setosa)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a perceptron classifier\n",
        "perceptron = Perceptron()\n",
        "\n",
        "# Train the perceptron on the training data\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = perceptron.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_binary, cmap=plt.cm.Paired, edgecolors='k')\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.title('Perceptron: Iris Binary Classification')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkCrYVijhNzs"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8IlIS-chDFU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Load the Titanic dataset\n",
        "uploaded = files.upload()\n",
        "titanic = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Drop rows with missing values and select relevant features\n",
        "titanic = titanic.dropna(subset=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived'])\n",
        "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic['Survived']\n",
        "\n",
        "# Convert categorical features to numerical using one-hot encoding\n",
        "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = logreg.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tiwM3DFh-Et"
      },
      "source": [
        "## Introducing ensemble methods\n",
        "Ensemble methods are machine learning algorithms that combine multiple predictive models to improve the accuracy and stability of predictions.\n",
        "like Random forrest which is combining of multiple decision tree classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbO47WhahoiR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgM75SMSisvb"
      },
      "source": [
        "## Bagging\n",
        "a machine learning technique that uses multiple models to improve the accuracy and stability of predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "720iSU0qiiaI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer Wisconsin dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier with 20 trees\n",
        "rf_classifier = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GN3mr7jDVY"
      },
      "source": [
        "## Boosting\n",
        "Boosting is a machine learning technique that improves the accuracy of predictive models by combining multiple weak learners into a single strong learner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvbBGhGui4kd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost classifier with 20 weak learners (Decision Trees)\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQSbGvPhjZZ2"
      },
      "source": [
        "## Random forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_4YeUXRjPZ7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer Wisconsin dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier with 100 trees\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_classifier.predict(X_test)\n",
        "\n",
        "# Display the accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wHr9XPUMhtBP",
        "ckJTZVsgkV7x",
        "mlb56Db7z_Hh",
        "iTaXYp0o0szA",
        "CsfFAxWK2lCS",
        "cCfXU-i_5GX8"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}